---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: kimi-k2-vllm
  namespace: llm-models
  labels:
    app: kimi-k2
    component: inference-server
  annotations:
    description: "External route for Kimi K2 vLLM API"
    # Increase timeout for long-running LLM requests
    haproxy.router.openshift.io/timeout: 3h
spec:
  # Uncomment and set your desired hostname
  # host: kimi-k2.apps.your-cluster.example.com
  to:
    kind: Service
    name: kimi-k2-vllm
    weight: 100
  port:
    targetPort: http
  tls:
    # Use edge termination for HTTPS
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
